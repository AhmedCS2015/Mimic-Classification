{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.17","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":862627,"sourceType":"datasetVersion","datasetId":457824}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"colab":{"name":"Final MIMIC","provenance":[]}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install xgboost lightgbm ","metadata":{"_uuid":"5865bbb6-2b70-4a1a-b6e4-86dc1e318a99","_cell_guid":"de701390-e35e-4f48-92b9-4dbc74ca42c0","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"id":"gMxqCwmGhtXJ"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\ndata = pd.read_csv('../input/mimic3c/mimic3c.csv')\nprint(\"With id\", data.shape)","metadata":{"_uuid":"7cfdf379-3555-488b-a318-48261863c885","_cell_guid":"403e5163-ea5a-4aac-9168-fda34ce65f5c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"id":"7d48y6HXhtXK","execution":{"iopub.status.busy":"2025-06-03T14:53:12.713559Z","iopub.execute_input":"2025-06-03T14:53:12.713987Z","iopub.status.idle":"2025-06-03T14:53:13.074472Z","shell.execute_reply.started":"2025-06-03T14:53:12.713952Z","shell.execute_reply":"2025-06-03T14:53:13.068731Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(data.shape)\nprint(data.head())","metadata":{"_uuid":"1f755833-9803-4ef3-8996-623bb13d5318","_cell_guid":"c38a1f24-fed4-4447-9b5a-38bbd91e321a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"id":"tEiOLctLhtXM","execution":{"iopub.status.busy":"2025-06-03T14:53:13.075341Z","iopub.execute_input":"2025-06-03T14:53:13.0756Z","iopub.status.idle":"2025-06-03T14:53:13.105907Z","shell.execute_reply.started":"2025-06-03T14:53:13.075575Z","shell.execute_reply":"2025-06-03T14:53:13.09842Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Load the dataset\ndata = pd.read_csv('../input/mimic3c/mimic3c.csv')\n\n# Initial exploration\nprint(\"Dataset Shape:\", data.shape)\nprint(\"\\n\" + \"=\"*50)\nprint(\"COLUMN INFORMATION:\")\nprint(\"=\"*50)\nprint(data.info())\nprint(\"\\n\" + \"=\"*50)\nprint(\"FIRST 5 ROWS:\")\nprint(\"=\"*50)\nprint(data.head())\nprint(\"\\n\" + \"=\"*50)\nprint(\"BASIC STATISTICS:\")\nprint(\"=\"*50)\nprint(data.describe())\nprint(\"\\n\" + \"=\"*50)\nprint(\"MISSING VALUES:\")\nprint(\"=\"*50)\nmissing_values = data.isnull().sum()\nmissing_percent = (missing_values / len(data)) * 100\nmissing_info = pd.DataFrame({\n    'Missing_Count': missing_values,\n    'Missing_Percentage': missing_percent\n})\nprint(missing_info[missing_info['Missing_Count'] > 0].sort_values('Missing_Count', ascending=False))\nprint(\"\\n\" + \"=\"*50)\nprint(\"UNIQUE VALUES PER COLUMN:\")\nprint(\"=\"*50)\nfor col in data.columns:\n    print(f\"{col}: {data[col].nunique()} unique values\")\nprint(\"\\n\" + \"=\"*50)\nprint(\"TARGET VARIABLE DISTRIBUTION (if identifiable):\")\nprint(\"=\"*50)\n# Check for potential target columns\npotential_targets = []\nfor col in data.columns:\n    if data[col].dtype == 'object' and data[col].nunique() <= 10:\n        potential_targets.append(col)\n    elif data[col].dtype in ['int64', 'float64'] and data[col].nunique() <= 10:\n        potential_targets.append(col)\n\nif potential_targets:\n    print(\"Potential target variables:\")\n    for target in potential_targets:\n        print(f\"\\n{target}:\")\n        print(data[target].value_counts())\nelse:\n    print(\"No obvious categorical target variable found. Please specify the target column.\")","metadata":{"_uuid":"49021861-424b-43d3-b34f-5df3989aa8e1","_cell_guid":"7a7c328d-daa6-4c6c-b3ef-45fc2903290b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"id":"6nwJDAv-htXN","execution":{"iopub.status.busy":"2025-06-03T14:53:13.107846Z","iopub.execute_input":"2025-06-03T14:53:13.108074Z","iopub.status.idle":"2025-06-03T14:53:13.604514Z","shell.execute_reply.started":"2025-06-03T14:53:13.108052Z","shell.execute_reply":"2025-06-03T14:53:13.598791Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 2: Comprehensive EDA\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set style for better plots\nplt.style.use('default')\nsns.set_palette(\"husl\")\n\n# Define target variable\ntarget = 'ExpiredHospital'\nprint(f\"TARGET VARIABLE: {target}\")\nprint(\"=\"*60)\n\n# Target distribution\nprint(\"TARGET DISTRIBUTION:\")\ntarget_counts = data[target].value_counts()\ntarget_pct = data[target].value_counts(normalize=True) * 100\nprint(f\"Survived (0): {target_counts[0]} ({target_pct[0]:.2f}%)\")\nprint(f\"Expired (1): {target_counts[1]} ({target_pct[1]:.2f}%)\")\nprint(f\"Class imbalance ratio: {target_counts[0]/target_counts[1]:.2f}:1\")\n\n# Create comprehensive EDA plots\nfig, axes = plt.subplots(3, 3, figsize=(18, 15))\nfig.suptitle('MIMIC-III Dataset - Comprehensive EDA', fontsize=16, fontweight='bold')\n\n# 1. Target distribution\naxes[0,0].pie(target_counts.values, labels=['Survived', 'Expired'], autopct='%1.1f%%',\n              colors=['lightblue', 'salmon'])\naxes[0,0].set_title('Hospital Mortality Distribution')\n\n# 2. Age distribution by target\ndata.boxplot(column='age', by=target, ax=axes[0,1])\naxes[0,1].set_title('Age Distribution by Mortality')\naxes[0,1].set_xlabel('Expired Hospital')\n\n# 3. Length of Stay by target\ndata.boxplot(column='LOSdays', by=target, ax=axes[0,2])\naxes[0,2].set_title('Length of Stay by Mortality')\naxes[0,2].set_xlabel('Expired Hospital')\n\n# 4. Gender distribution by target\ngender_crosstab = pd.crosstab(data['gender'], data[target], normalize='index') * 100\ngender_crosstab.plot(kind='bar', ax=axes[1,0], color=['lightblue', 'salmon'])\naxes[1,0].set_title('Mortality Rate by Gender')\naxes[1,0].set_ylabel('Percentage')\naxes[1,0].legend(['Survived', 'Expired'])\naxes[1,0].tick_params(axis='x', rotation=0)\n\n# 5. Admit type distribution by target\nadmit_crosstab = pd.crosstab(data['admit_type'], data[target], normalize='index') * 100\nadmit_crosstab.plot(kind='bar', ax=axes[1,1], color=['lightblue', 'salmon'])\naxes[1,1].set_title('Mortality Rate by Admit Type')\naxes[1,1].set_ylabel('Percentage')\naxes[1,1].legend(['Survived', 'Expired'])\naxes[1,1].tick_params(axis='x', rotation=45)\n\n# 6. Insurance type by target\ninsurance_crosstab = pd.crosstab(data['insurance'], data[target], normalize='index') * 100\ninsurance_crosstab.plot(kind='bar', ax=axes[1,2], color=['lightblue', 'salmon'])\naxes[1,2].set_title('Mortality Rate by Insurance')\naxes[1,2].set_ylabel('Percentage')\naxes[1,2].legend(['Survived', 'Expired'])\naxes[1,2].tick_params(axis='x', rotation=45)\n\n# 7. Number of diagnoses distribution\naxes[2,0].hist([data[data[target]==0]['NumDiagnosis'], data[data[target]==1]['NumDiagnosis']],\n               bins=30, alpha=0.7, label=['Survived', 'Expired'], color=['lightblue', 'salmon'])\naxes[2,0].set_title('Number of Diagnoses Distribution')\naxes[2,0].set_xlabel('Number of Diagnoses')\naxes[2,0].set_ylabel('Frequency')\naxes[2,0].legend()\naxes[2,0].set_xlim(0, 20)\n\n# 8. Total interactions distribution\naxes[2,1].hist([data[data[target]==0]['TotalNumInteract'], data[data[target]==1]['TotalNumInteract']],\n               bins=30, alpha=0.7, label=['Survived', 'Expired'], color=['lightblue', 'salmon'])\naxes[2,1].set_title('Total Interactions Distribution')\naxes[2,1].set_xlabel('Total Number of Interactions')\naxes[2,1].set_ylabel('Frequency')\naxes[2,1].legend()\naxes[2,1].set_xlim(0, 2000)\n\n# 9. Chart events distribution\naxes[2,2].hist([data[data[target]==0]['NumChartEvents'], data[data[target]==1]['NumChartEvents']],\n               bins=30, alpha=0.7, label=['Survived', 'Expired'], color=['lightblue', 'salmon'])\naxes[2,2].set_title('Chart Events Distribution')\naxes[2,2].set_xlabel('Number of Chart Events')\naxes[2,2].set_ylabel('Frequency')\naxes[2,2].legend()\naxes[2,2].set_xlim(0, 1500)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"KEY INSIGHTS FROM EDA:\")\nprint(\"=\"*60)\n\n# Age analysis\nage_survived = data[data[target]==0]['age'].mean()\nage_expired = data[data[target]==1]['age'].mean()\nprint(f\"Average age - Survived: {age_survived:.1f}, Expired: {age_expired:.1f}\")\n\n# LOS analysis\nlos_survived = data[data[target]==0]['LOSdays'].mean()\nlos_expired = data[data[target]==1]['LOSdays'].mean()\nprint(f\"Average LOS - Survived: {los_survived:.1f} days, Expired: {los_expired:.1f} days\")\n\n# Numerical features correlation with target\nprint(\"\\nCORRELATION WITH TARGET (ExpiredHospital):\")\nnumerical_cols = data.select_dtypes(include=[np.number]).columns.tolist()\nnumerical_cols.remove(target)\nnumerical_cols.remove('hadm_id')  # Remove ID column\n\ncorrelations = data[numerical_cols + [target]].corr()[target].sort_values(key=abs, ascending=False)[1:]\nprint(correlations.head(10))\n\n# Missing values impact\nprint(f\"\\nMISSING VALUES ANALYSIS:\")\nprint(f\"Marital Status missing: {data['marital_status'].isnull().sum()} ({data['marital_status'].isnull().sum()/len(data)*100:.1f}%)\")\nprint(f\"Religion missing: {data['religion'].isnull().sum()} ({data['religion'].isnull().sum()/len(data)*100:.1f}%)\")\nprint(f\"Admit Diagnosis missing: {data['AdmitDiagnosis'].isnull().sum()} ({data['AdmitDiagnosis'].isnull().sum()/len(data)*100:.1f}%)\")\n\n# High-level summary\nprint(f\"\\nDATASET SUMMARY:\")\nprint(f\"- Total samples: {len(data):,}\")\nprint(f\"- Features: {len(data.columns)-1}\")\nprint(f\"- Target: Hospital Mortality (Binary)\")\nprint(f\"- Class distribution: {target_pct[0]:.1f}% survived, {target_pct[1]:.1f}% expired\")\nprint(f\"- Imbalance ratio: {target_counts[0]/target_counts[1]:.1f}:1 (moderate imbalance)\")","metadata":{"_uuid":"fc4f1431-5e83-4381-922e-1c3da9c232d5","_cell_guid":"281d0894-a130-4639-8919-0a0383a34444","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"id":"09329dZzhtXP","execution":{"iopub.status.busy":"2025-06-03T14:53:13.606903Z","iopub.execute_input":"2025-06-03T14:53:13.607202Z","iopub.status.idle":"2025-06-03T14:53:15.685359Z","shell.execute_reply.started":"2025-06-03T14:53:13.607174Z","shell.execute_reply":"2025-06-03T14:53:15.679536Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Default Models**","metadata":{"_uuid":"8417930f-7394-4ddd-9cdd-9e640df6a394","_cell_guid":"0bed155b-24fb-40a3-ae77-5efd3d311bb7","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Complete MIMIC-III Classification Analysis Pipeline\n# This script includes data loading, preprocessing, and the classification models\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import (classification_report, confusion_matrix, roc_auc_score, \n                           roc_curve, precision_recall_curve, average_precision_score,\n                           f1_score, precision_score, recall_score, accuracy_score)\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nimport warnings\nwarnings.filterwarnings('ignore')\nimport xgboost as xgb\nprint(\"MIMIC-III COMPLETE CLASSIFICATION ANALYSIS PIPELINE\")\nprint(\"=\"*80)\n\n# STEP 1: DATA LOADING AND EXPLORATION\nprint(\"STEP 1: DATA LOADING AND EXPLORATION\")\nprint(\"-\" * 40)\n\n# Load the dataset - change this path to match your data location\n# Common paths: 'mimic3c.csv', 'data/mimic3c.csv', '../input/mimic3c/mimic3c.csv'\ndata = pd.read_csv('../input/mimic3c/mimic3c.csv')  # Update this path as needed\n# Alternative: data = pd.read_csv('mimic3c.csv')  # If file is in current directory\nprint(f\"Dataset loaded successfully!\")\nprint(f\"Shape: {data.shape}\")\nprint(f\"Columns: {list(data.columns)}\")\n\n# Display first few rows\nprint(\"\\nFirst 5 rows:\")\nprint(data.head())\n\n# Basic information about the dataset\nprint(f\"\\nDataset Info:\")\nprint(data.info())\n\n# Check for missing values\nprint(f\"\\nMissing Values:\")\nmissing_info = data.isnull().sum()\nmissing_percent = (missing_info / len(data)) * 100\nmissing_df = pd.DataFrame({\n    'Missing_Count': missing_info,\n    'Missing_Percentage': missing_percent\n})\nprint(missing_df[missing_df['Missing_Count'] > 0].sort_values('Missing_Count', ascending=False))\n\n# STEP 2: FEATURE SELECTION AND TARGET PREPARATION\nprint(f\"\\n\" + \"=\"*80)\nprint(\"STEP 2: FEATURE SELECTION AND TARGET PREPARATION\")\nprint(\"-\" * 40)\n\n# Select features for modeling (as specified)\nfeature_columns = [\n    # Demographics\n    'age', 'gender_encoded', 'ethnicity_encoded',\n    # Admission details\n    'admit_type_encoded', 'admit_location_encoded', 'insurance_encoded',\n    'religion_encoded', 'marital_status_encoded',\n    # Clinical features\n    'LOSdays', 'NumDiagnosis', 'NumProcs', 'NumCallouts',\n    'NumCPTevents', 'NumInput', 'NumLabs', 'NumMicroLabs',\n    'NumNotes', 'NumOutput', 'NumRx', 'NumProcEvents',\n    'NumTransfers', 'NumChartEvents', 'TotalNumInteract',\n    # Encoded diagnosis and procedure\n    'AdmitDiagnosis_encoded', 'AdmitProcedure_encoded',\n    # Engineered features\n    'age_group_encoded', 'los_category_encoded',\n    'lab_intensity', 'med_intensity', 'procedure_intensity', 'monitoring_intensity',\n    'clinical_burden', 'interaction_density',\n    'high_age', 'high_los', 'high_diagnoses', 'emergency_admit', 'icu_indicator'\n]\n\n# Target variable\ntarget_column = 'ExpiredHospital'\n\nprint(f\"âœ… Target variable: {target_column}\")\nprint(f\"ðŸ“Š Total features selected: {len(feature_columns)}\")\n\n# Check if all required columns exist in the dataset\nmissing_features = [col for col in feature_columns if col not in data.columns]\nmissing_target = target_column not in data.columns\n\nif missing_features:\n    print(f\"âš ï¸  Missing feature columns: {missing_features}\")\n    print(\"Available columns in dataset:\")\n    print(list(data.columns))\n    print(\"\\nFiltering to only available features...\")\n    feature_columns = [col for col in feature_columns if col in data.columns]\n    print(f\"âœ… Using {len(feature_columns)} available features\")\n\nif missing_target:\n    print(f\"âŒ Target column '{target_column}' not found in dataset!\")\n    print(\"Available columns:\", list(data.columns))\n    print(\"No\")\n\n# Create feature matrix and target vector\nX = data[feature_columns].copy()\ny = data[target_column].copy()\n\nprint(f\"\\nðŸ“Š FEATURE CATEGORIES:\")\nprint(\"-\" * 30)\n\n# Categorize features for better understanding\ndemographics = ['age', 'gender_encoded', 'ethnicity_encoded']\nadmission_details = ['admit_type_encoded', 'admit_location_encoded', 'insurance_encoded', \n                    'religion_encoded', 'marital_status_encoded']\nclinical_features = ['LOSdays', 'NumDiagnosis', 'NumProcs', 'NumCallouts', 'NumCPTevents', \n                    'NumInput', 'NumLabs', 'NumMicroLabs', 'NumNotes', 'NumOutput', 'NumRx', \n                    'NumProcEvents', 'NumTransfers', 'NumChartEvents', 'TotalNumInteract']\nencoded_features = ['AdmitDiagnosis_encoded', 'AdmitProcedure_encoded']\nengineered_features = ['age_group_encoded', 'los_category_encoded', 'lab_intensity', \n                      'med_intensity', 'procedure_intensity', 'monitoring_intensity',\n                      'clinical_burden', 'interaction_density', 'high_age', 'high_los', \n                      'high_diagnoses', 'emergency_admit', 'icu_indicator']\n\nprint(f\"Demographics ({len([f for f in demographics if f in feature_columns])}): {[f for f in demographics if f in feature_columns]}\")\nprint(f\"Admission Details ({len([f for f in admission_details if f in feature_columns])}): {[f for f in admission_details if f in feature_columns]}\")\nprint(f\"Clinical Features ({len([f for f in clinical_features if f in feature_columns])}): {[f for f in clinical_features if f in feature_columns]}\")\nprint(f\"Encoded Features ({len([f for f in encoded_features if f in feature_columns])}): {[f for f in encoded_features if f in feature_columns]}\")\nprint(f\"Engineered Features ({len([f for f in engineered_features if f in feature_columns])}): {[f for f in engineered_features if f in feature_columns]}\")\n\nprint(f\"\\nðŸ“ˆ DATA SUMMARY:\")\nprint(f\"   Features shape: {X.shape}\")\nprint(f\"   Target shape: {y.shape}\")\nprint(f\"   Target distribution:\\n{y.value_counts()}\")\n\n# Check for missing values in selected features\nmissing_in_features = X.isnull().sum()\nif missing_in_features.sum() > 0:\n    print(f\"\\nâš ï¸  Missing values in features:\")\n    print(missing_in_features[missing_in_features > 0])\n    \n    # Simple imputation for missing values\n    print(\"Applying simple imputation...\")\n    X = X.fillna(X.median())\n    print(\"âœ… Missing values filled with median\")\n\nprint(f\"\\nâœ… Feature matrix and target ready for modeling!\")\nprint(f\"   Final X shape: {X.shape}\")\nprint(f\"   Final y shape: {y.shape}\")\nprint(f\"   Target classes: {sorted(y.unique())}\")\n\n# STEP 4: BASIC CLASSIFICATION MODELS WITH DEFAULT PARAMETERS\nprint(f\"\\n\" + \"=\"*80)\nprint(\"STEP 4: BASIC CLASSIFICATION MODELS WITH DEFAULT PARAMETERS\")\nprint(\"=\"*80)\n\n# 1. TRAIN-TEST SPLIT (STRATIFIED)\nprint(\"1. SPLITTING DATA...\")\nprint(\"-\" * 40)\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\nprint(f\"Training set: {X_train.shape[0]} samples\")\nprint(f\"Test set: {X_test.shape[0]} samples\")\nprint(f\"Training class distribution: {y_train.value_counts().to_dict()}\")\nprint(f\"Test class distribution: {y_test.value_counts().to_dict()}\")\n\n\nprint(\"-\" * 40)\n\n\nX_train_scaled =(X_train)\nX_test_scaled = (X_test)\n\n\n\n# 3. DEFINE MODELS WITH DEFAULT PARAMETERS\nprint(\"\\n3. DEFINING MODELS WITH DEFAULT PARAMETERS...\")\nprint(\"-\" * 40)\n\n# All models with completely default parameters\nmodels = {\n    'Logistic Regression': LogisticRegression(random_state=42),\n    'Random Forest': RandomForestClassifier(random_state=42),\n    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n    'Neural Network (MLP)': MLPClassifier(random_state=42, max_iter=200),\n    'Support Vector Machine': SVC(probability=True, random_state=42),\n    'Naive Bayes': GaussianNB(),\n    'Decision Tree': DecisionTreeClassifier(random_state=42),\n    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42),\n    'LightGBM': LGBMClassifier(random_state=42)\n}\nprint(\"Models defined with default parameters only (Neural Network max_iter=200 to ensure convergence)\")\n\n# 4. TRAIN MODELS AND COLLECT RESULTS\nmodel_results = {}\ntrained_models = {}\n\n# Create figure for ROC curves\nplt.figure(figsize=(10, 8))\ncolors = ['blue', 'green', 'red', 'purple', 'orange', 'cyan', 'magenta', 'brown', 'black']\n\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"MODEL TRAINING AND EVALUATION RESULTS\")\nprint(\"=\"*80)\n\nfor i, (name, model) in enumerate(models.items()):\n    print(f\"\\nðŸ”¹ TRAINING: {name}\")\n    print(\"-\" * 60)\n    \n    # Train model with default parameters on scaled data\n    model.fit(X_train_scaled, y_train)\n    \n    # Cross-validation on training set\n    cv_scores = cross_val_score(model, X_train_scaled, y_train, \n                               cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n                               scoring='roc_auc', n_jobs=-1)\n    \n    # Predictions on test set\n    y_pred = model.predict(X_test_scaled)\n    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n    \n    # Calculate all metrics\n    accuracy = accuracy_score(y_test, y_pred)\n    precision = precision_score(y_test, y_pred)\n    recall = recall_score(y_test, y_pred)\n    f1 = f1_score(y_test, y_pred)\n    auc_score = roc_auc_score(y_test, y_pred_proba)\n    \n    # Store results\n    model_results[name] = {\n        'CV_AUC_Mean': cv_scores.mean(),\n        'CV_AUC_Std': cv_scores.std(),\n        'Accuracy': accuracy,\n        'Precision': precision,\n        'Recall': recall,\n        'F1_Score': f1,\n        'AUC': auc_score,\n        'Predictions': y_pred,\n        'Probabilities': y_pred_proba\n    }\n    \n    trained_models[name] = model\n    \n    # Print detailed metrics for this model\n    print(f\"ðŸ“Š PERFORMANCE METRICS:\")\n    print(f\"   âœ… Accuracy:     {accuracy:.4f}\")\n    print(f\"   âœ… Precision:    {precision:.4f}\")\n    print(f\"   âœ… Recall:       {recall:.4f}\")\n    print(f\"   âœ… F1-Score:     {f1:.4f}\")\n    print(f\"   âœ… AUC-ROC:      {auc_score:.4f}\")\n    print(f\"   âœ… CV AUC:       {cv_scores.mean():.4f} (+/- {cv_scores.std()*2:.4f})\")\n    \n    # Calculate and plot ROC curve\n    fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n    colors = ['blue', 'green', 'red', 'purple', 'orange', 'cyan', 'magenta', 'brown', 'black']\n    # Plot ROC curve\n    plt.plot(fpr, tpr, linewidth=2, color=colors[i], \n             label=f'{name} (AUC: {auc_score:.3f})')\n\n# Finalize ROC plot\nplt.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Random Classifier')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate', fontsize=12)\nplt.ylabel('True Positive Rate', fontsize=12)\nplt.title('ROC Curves - Default Parameter Models', fontsize=14, fontweight='bold')\nplt.legend(loc=\"lower right\", fontsize=10)\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n# 5. MODEL COMPARISON TABLE\nprint(\"\\n\" + \"=\"*80)\nprint(\"MODEL COMPARISON - DEFAULT PARAMETERS\")\nprint(\"=\"*80)\n\n# Create comparison DataFrame\ncomparison_df = pd.DataFrame(model_results).T\ncomparison_df = comparison_df[['Accuracy', 'Precision', 'Recall', 'F1_Score', 'AUC', 'CV_AUC_Mean']].round(4)\n\nprint(comparison_df.to_string())\n\n# Find best model for each metric\nprint(f\"\\nðŸ† BEST PERFORMERS BY METRIC:\")\nprint(f\"   ðŸ¥‡ Best Accuracy:  {comparison_df['Accuracy'].idxmax()} ({comparison_df['Accuracy'].max():.4f})\")\nprint(f\"   ðŸ¥‡ Best Precision: {comparison_df['Precision'].idxmax()} ({comparison_df['Precision'].max():.4f})\")\nprint(f\"   ðŸ¥‡ Best Recall:    {comparison_df['Recall'].idxmax()} ({comparison_df['Recall'].max():.4f})\")\nprint(f\"   ðŸ¥‡ Best F1-Score:  {comparison_df['F1_Score'].idxmax()} ({comparison_df['F1_Score'].max():.4f})\")\nprint(f\"   ðŸ¥‡ Best AUC:       {comparison_df['AUC'].idxmax()} ({comparison_df['AUC'].max():.4f})\")\n\n# Overall best model (based on AUC)\nbest_model_name = comparison_df['AUC'].idxmax()\nprint(f\"\\nðŸŽ¯ OVERALL BEST MODEL: {best_model_name}\")\nprint(f\"   ðŸ“ˆ AUC Score: {comparison_df.loc[best_model_name, 'AUC']:.4f}\")\n\n# 6. DETAILED EVALUATION OF BEST MODEL\nprint(f\"\\n\" + \"=\"*80)\nprint(f\"DETAILED ANALYSIS - {best_model_name}\")\nprint(\"=\"*80)\n\nbest_predictions = model_results[best_model_name]['Predictions']\nbest_probabilities = model_results[best_model_name]['Probabilities']\n\n# Classification Report\nprint(\"ðŸ“‹ CLASSIFICATION REPORT:\")\ntarget_names = ['Class 0', 'Class 1']  # Adjust based on your target\nprint(classification_report(y_test, best_predictions, target_names=target_names, digits=4))\n\n# Confusion Matrix\nprint(\"\\nðŸ“Š CONFUSION MATRIX:\")\ncm = confusion_matrix(y_test, best_predictions)\nprint(cm)\n\n# Calculate additional metrics\ntn, fp, fn, tp = cm.ravel()\nspecificity = tn / (tn + fp)\nsensitivity = tp / (tp + fn)\nppv = tp / (tp + fp) if (tp + fp) > 0 else 0\nnpv = tn / (tn + fn) if (tn + fn) > 0 else 0\n\nprint(f\"\\nðŸ“ˆ ADDITIONAL METRICS:\")\nprint(f\"   âœ… Sensitivity (True Positive Rate): {sensitivity:.4f}\")\nprint(f\"   âœ… Specificity (True Negative Rate): {specificity:.4f}\")\nprint(f\"   âœ… Positive Predictive Value (PPV):  {ppv:.4f}\")\nprint(f\"   âœ… Negative Predictive Value (NPV):  {npv:.4f}\")\n\n# 7. VISUALIZATION - CONFUSION MATRIX HEATMAP\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n            xticklabels=['Class 0', 'Class 1'], yticklabels=['Class 0', 'Class 1'])\nplt.title(f'Confusion Matrix - {best_model_name}')\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\nplt.tight_layout()\nplt.show()\n\n# 8. FINAL SUMMARY\nprint(f\"\\n\" + \"=\"*80)\nprint(\"FINAL SUMMARY - DEFAULT PARAMETER MODELS\")\nprint(\"=\"*80)\n\n# Create a summary table\nsummary_data = []\nfor model_name, results in model_results.items():\n    summary_data.append({\n        'Model': model_name,\n        'Accuracy': f\"{results['Accuracy']:.4f}\",\n        'Precision': f\"{results['Precision']:.4f}\",\n        'Recall': f\"{results['Recall']:.4f}\",\n        'F1-Score': f\"{results['F1_Score']:.4f}\",\n        'AUC-ROC': f\"{results['AUC']:.4f}\",\n        'CV AUC': f\"{results['CV_AUC_Mean']:.4f}\"\n    })\n\nsummary_df = pd.DataFrame(summary_data)\nprint(summary_df.to_string(index=False))\n\nprint(f\"\\nðŸ“ MODEL PARAMETERS USED:\")\nfor name, model in trained_models.items():\n    print(f\"\\n{name}:\")\n    print(f\"   Parameters: {model.get_params()}\")\n\nprint(f\"\\nðŸŽ¯ SUMMARY:\")\nprint(f\"   ðŸ“Š Total models trained: {len(model_results)}\")\nprint(f\"   ðŸ† Best model: {best_model_name}\")\nprint(f\"   ðŸ“ˆ Best AUC: {comparison_df['AUC'].max():.4f}\")\nprint(f\"   âš™ï¸  All models used default parameters\")\nprint(f\"   ðŸš« No class balancing or optimization applied\")\nprint(f\"   ðŸ“‹ All standard metrics calculated and displayed\")\nprint(f\"   ðŸŽ² Target variable: {target_column}\")\nprint(f\"   ðŸ“Š Dataset: {data.shape[0]} samples, {X.shape[1]} features\")\n\nprint(f\"\\n\" + \"=\"*80)\nprint(\"ANALYSIS COMPLETE!\")\nprint(\"=\"*80)","metadata":{"_uuid":"c6eccf6c-1f48-4174-9321-9193f5f8095d","_cell_guid":"b5bd73c5-e19e-4848-aed4-1082449875ef","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-06-03T14:53:15.687682Z","iopub.execute_input":"2025-06-03T14:53:15.687999Z","iopub.status.idle":"2025-06-03T15:11:03.409461Z","shell.execute_reply.started":"2025-06-03T14:53:15.68797Z","shell.execute_reply":"2025-06-03T15:11:03.403927Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Improvement**","metadata":{"_uuid":"4b4d8d76-d07b-491e-8a8a-d826a0bba628","_cell_guid":"b2368eff-7bd6-495f-8bf7-c74a97d6eb27","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Step 3: Data Preprocessing and Feature Engineering\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.impute import SimpleImputer\nimport pandas as pd\nimport numpy as np\n\nprint(\"STEP 3: DATA PREPROCESSING & FEATURE ENGINEERING\")\nprint(\"=\"*60)\n\n# Create a copy for preprocessing\ndf = data.copy()\nprint(f\"Original dataset shape: {df.shape}\")\n\n# 1. HANDLE MISSING VALUES\nprint(\"\\n1. HANDLING MISSING VALUES...\")\nprint(\"-\" * 40)\n\n# Fill missing marital_status with mode\ndf['marital_status'].fillna(df['marital_status'].mode()[0], inplace=True)\n\n# Fill missing religion with 'UNKNOWN'\ndf['religion'].fillna('UNKNOWN', inplace=True)\n\n# Fill missing AdmitDiagnosis with mode\ndf['AdmitDiagnosis'].fillna(df['AdmitDiagnosis'].mode()[0], inplace=True)\n\nprint(\"Missing values after imputation:\")\nprint(df.isnull().sum().sum())\n\n# 2. FEATURE ENGINEERING\nprint(\"\\n2. FEATURE ENGINEERING...\")\nprint(\"-\" * 40)\n\n# Age groups (clinical relevance)\ndf['age_group'] = pd.cut(df['age'],\n                        bins=[0, 18, 35, 50, 65, 80, 100],\n                        labels=['pediatric', 'young_adult', 'adult', 'middle_aged', 'elderly', 'very_elderly'])\n\n# LOS categories\ndf['los_category'] = pd.cut(df['LOSdays'],\n                           bins=[0, 2, 7, 14, 30, 300],\n                           labels=['very_short', 'short', 'medium', 'long', 'very_long'])\n\n# Clinical activity intensity features\ndf['lab_intensity'] = df['NumLabs'] / (df['LOSdays'] + 1)  # +1 to avoid division by zero\ndf['med_intensity'] = df['NumRx'] / (df['LOSdays'] + 1)\ndf['procedure_intensity'] = df['NumProcs'] / (df['LOSdays'] + 1)\ndf['monitoring_intensity'] = df['NumChartEvents'] / (df['LOSdays'] + 1)\n\n# Total clinical burden score\ndf['clinical_burden'] = (df['NumDiagnosis'] + df['NumProcs'] + df['NumCallouts'])\n\n# Interaction density\ndf['interaction_density'] = df['TotalNumInteract'] / (df['LOSdays'] + 1)\n\n# High-risk indicators\ndf['high_age'] = (df['age'] >= 70).astype(int)\ndf['high_los'] = (df['LOSdays'] >= 14).astype(int)\ndf['high_diagnoses'] = (df['NumDiagnosis'] >= df['NumDiagnosis'].quantile(0.75)).astype(int)\ndf['emergency_admit'] = (df['admit_type'] == 'EMERGENCY').astype(int)\n\n# ICU indicators (based on high monitoring)\ndf['icu_indicator'] = (df['NumChartEvents'] >= df['NumChartEvents'].quantile(0.90)).astype(int)\n\nprint(f\"New features created: {len(df.columns) - len(data.columns)}\")\nprint(\"New feature list:\")\nnew_features = [col for col in df.columns if col not in data.columns]\nfor feature in new_features:\n    print(f\"  - {feature}\")\n\n# 3. ENCODE CATEGORICAL VARIABLES\nprint(\"\\n3. ENCODING CATEGORICAL VARIABLES...\")\nprint(\"-\" * 40)\n\n# Initialize label encoders\nlabel_encoders = {}\n\n# Categorical columns to encode\ncategorical_cols = ['gender', 'admit_type', 'admit_location', 'insurance',\n                   'religion', 'marital_status', 'ethnicity', 'age_group', 'los_category']\n\n# Apply label encoding\nfor col in categorical_cols:\n    le = LabelEncoder()\n    df[col + '_encoded'] = le.fit_transform(df[col])\n    label_encoders[col] = le\n    print(f\"  {col}: {len(le.classes_)} categories -> encoded\")\n\n# 4. HANDLE HIGH CARDINALITY FEATURES\nprint(\"\\n4. HANDLING HIGH CARDINALITY FEATURES...\")\nprint(\"-\" * 40)\n\n# AdmitDiagnosis and AdmitProcedure have too many unique values\n# Create frequency-based encoding\ndiagnosis_freq = df['AdmitDiagnosis'].value_counts()\nprocedure_freq = df['AdmitProcedure'].value_counts()\n\n# Keep top 50 most frequent, group others as 'OTHER'\ntop_diagnoses = diagnosis_freq.head(50).index\ntop_procedures = procedure_freq.head(50).index\n\ndf['AdmitDiagnosis_grouped'] = df['AdmitDiagnosis'].apply(\n    lambda x: x if x in top_diagnoses else 'OTHER'\n)\ndf['AdmitProcedure_grouped'] = df['AdmitProcedure'].apply(\n    lambda x: x if x in top_procedures else 'OTHER'\n)\n\n# Encode the grouped versions\nle_diag = LabelEncoder()\nle_proc = LabelEncoder()\ndf['AdmitDiagnosis_encoded'] = le_diag.fit_transform(df['AdmitDiagnosis_grouped'])\ndf['AdmitProcedure_encoded'] = le_proc.fit_transform(df['AdmitProcedure_grouped'])\n\nprint(f\"  AdmitDiagnosis: {len(diagnosis_freq)} -> {len(le_diag.classes_)} categories\")\nprint(f\"  AdmitProcedure: {len(procedure_freq)} -> {len(le_proc.classes_)} categories\")\n\n# 5. FEATURE SELECTION FOR MODELING\nprint(\"\\n5. PREPARING FEATURE SET...\")\nprint(\"-\" * 40)\n\n# Select features for modeling\nfeature_columns = [\n    # Demographics\n    'age', 'gender_encoded', 'ethnicity_encoded',\n\n    # Admission details\n    'admit_type_encoded', 'admit_location_encoded', 'insurance_encoded',\n    'religion_encoded', 'marital_status_encoded',\n\n    # Clinical features\n    'LOSdays', 'NumDiagnosis', 'NumProcs', 'NumCallouts',\n    'NumCPTevents', 'NumInput', 'NumLabs', 'NumMicroLabs',\n    'NumNotes', 'NumOutput', 'NumRx', 'NumProcEvents',\n    'NumTransfers', 'NumChartEvents', 'TotalNumInteract',\n\n    # Encoded diagnosis and procedure\n    'AdmitDiagnosis_encoded', 'AdmitProcedure_encoded',\n\n    # Engineered features\n    'age_group_encoded', 'los_category_encoded',\n    'lab_intensity', 'med_intensity', 'procedure_intensity', 'monitoring_intensity',\n    'clinical_burden', 'interaction_density',\n    'high_age', 'high_los', 'high_diagnoses', 'emergency_admit', 'icu_indicator'\n]\n\n# Create feature matrix\nX = df[feature_columns].copy()\ny = df['ExpiredHospital'].copy()\n\nprint(f\"Final feature set: {X.shape[1]} features\")\nprint(f\"Target distribution: {y.value_counts().to_dict()}\")\n\n# 6. HANDLE OUTLIERS (using IQR method for key continuous features)\nprint(\"\\n6. OUTLIER HANDLING...\")\nprint(\"-\" * 40)\n\ncontinuous_features = ['LOSdays', 'NumDiagnosis', 'NumInput', 'NumLabs', 'NumChartEvents',\n                      'TotalNumInteract', 'lab_intensity', 'med_intensity', 'monitoring_intensity']\n\noutlier_stats = {}\nfor feature in continuous_features:\n    Q1 = X[feature].quantile(0.25)\n    Q3 = X[feature].quantile(0.75)\n    IQR = Q3 - Q1\n    lower_bound = Q1 - 1.5 * IQR\n    upper_bound = Q3 + 1.5 * IQR\n\n    outliers_before = ((X[feature] < lower_bound) | (X[feature] > upper_bound)).sum()\n\n    # Cap outliers instead of removing them\n    X[feature] = np.clip(X[feature], lower_bound, upper_bound)\n\n    outlier_stats[feature] = outliers_before\n\nprint(\"Outliers capped per feature:\")\nfor feature, count in outlier_stats.items():\n    if count > 0:\n        print(f\"  {feature}: {count} outliers capped\")\n\n# 7. FINAL DATASET INFO\nprint(f\"\\n7. FINAL PREPROCESSED DATASET:\")\nprint(\"-\" * 40)\nprint(f\"Shape: {X.shape}\")\nprint(f\"Features: {list(X.columns)}\")\nprint(f\"Target class distribution:\")\nprint(f\"  Survived (0): {(y==0).sum()} ({(y==0).sum()/len(y)*100:.1f}%)\")\nprint(f\"  Expired (1): {(y==1).sum()} ({(y==1).sum()/len(y)*100:.1f}%)\")\nprint(f\"Missing values in X: {X.isnull().sum().sum()}\")\nprint(f\"Missing values in y: {y.isnull().sum()}\")\n\n# Check for any remaining issues\nprint(f\"\\nData quality check:\")\nprint(f\"- Infinite values: {np.isinf(X).sum().sum()}\")\nprint(f\"- NaN values: {X.isnull().sum().sum()}\")\nprint(f\"- Feature dtypes: {X.dtypes.value_counts().to_dict()}\")\n\nprint(\"\\nPreprocessing completed successfully!\")\nprint(\"Ready for model training...\")","metadata":{"_uuid":"336b5448-fb2f-4089-9218-6dd34ffa7fb6","_cell_guid":"3bd26a4d-bfab-4b2b-a6d4-6d9c7f2c5083","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"id":"nx_UXZ0ehtXQ","execution":{"iopub.status.busy":"2025-06-03T15:11:03.411651Z","iopub.execute_input":"2025-06-03T15:11:03.41212Z","iopub.status.idle":"2025-06-03T15:11:03.887922Z","shell.execute_reply.started":"2025-06-03T15:11:03.412093Z","shell.execute_reply":"2025-06-03T15:11:03.882797Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"##############################","metadata":{"_uuid":"4e9dcc22-9e9c-4356-b0f9-6a2fe8fbcfd5","_cell_guid":"520e3496-1c51-4bb5-8143-59b41e326ca7","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-06-03T15:11:03.888777Z","iopub.execute_input":"2025-06-03T15:11:03.890234Z","iopub.status.idle":"2025-06-03T15:11:03.901352Z","shell.execute_reply.started":"2025-06-03T15:11:03.890206Z","shell.execute_reply":"2025-06-03T15:11:03.894404Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 4: High-Performance Classification Model (Enhanced with Detailed Metrics)\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import (classification_report, confusion_matrix, roc_auc_score, \n                           roc_curve, precision_recall_curve, average_precision_score,\n                           f1_score, precision_score, recall_score, accuracy_score)\nfrom sklearn.utils.class_weight import compute_class_weight\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(\"STEP 4: HIGH-PERFORMANCE CLASSIFICATION MODEL\")\nprint(\"=\"*60)\n\n# 1. TRAIN-TEST SPLIT (STRATIFIED)\nprint(\"1. SPLITTING DATA...\")\nprint(\"-\" * 40)\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\nprint(f\"Training set: {X_train.shape[0]} samples\")\nprint(f\"Test set: {X_test.shape[0]} samples\")\nprint(f\"Training class distribution: {y_train.value_counts().to_dict()}\")\nprint(f\"Test class distribution: {y_test.value_counts().to_dict()}\")\n\n# 2. FEATURE SCALING\nprint(\"\\n2. FEATURE SCALING...\")\nprint(\"-\" * 40)\n\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nprint(\"Features scaled using StandardScaler\")\n\n# 3. CALCULATE CLASS WEIGHTS FOR IMBALANCE HANDLING\nprint(\"\\n3. HANDLING CLASS IMBALANCE...\")\nprint(\"-\" * 40)\n\n# Calculate class weights\nclass_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\nclass_weight_dict = {0: class_weights[0], 1: class_weights[1]}\nscale_pos_weight = class_weights[1] / class_weights[0]\n\nprint(f\"Class weights: {class_weight_dict}\")\nprint(f\"Scale pos weight: {scale_pos_weight:.3f}\")\n\n# Manual oversampling function (alternative to SMOTE)\ndef manual_oversample(X, y, random_state=42):\n    \"\"\"Simple random oversampling to balance classes\"\"\"\n    np.random.seed(random_state)\n    \n    # Separate majority and minority classes\n    majority_class = 0\n    minority_class = 1\n    \n    X_majority = X[y == majority_class]\n    X_minority = X[y == minority_class]\n    y_majority = y[y == majority_class]\n    y_minority = y[y == minority_class]\n    \n    # Calculate how many samples to add\n    n_majority = len(X_majority)\n    n_minority = len(X_minority)\n    n_to_add = n_majority - n_minority\n    \n    # Randomly sample from minority class with replacement\n    indices = np.random.choice(len(X_minority), size=n_to_add, replace=True)\n    X_minority_upsampled = X_minority[indices]\n    y_minority_upsampled = y_minority.iloc[indices]\n    \n    # Combine majority and upsampled minority\n    X_balanced = np.vstack([X_majority, X_minority, X_minority_upsampled])\n    y_balanced = np.hstack([y_majority, y_minority, y_minority_upsampled])\n    \n    return X_balanced, y_balanced\n\n# Apply manual oversampling\nX_train_balanced, y_train_balanced = manual_oversample(X_train_scaled, y_train, random_state=42)\n\nprint(f\"Before oversampling: {pd.Series(y_train).value_counts().to_dict()}\")\nprint(f\"After oversampling: {pd.Series(y_train_balanced).value_counts().to_dict()}\")\n\n# 4. MODEL TRAINING WITH MULTIPLE ALGORITHMS\nprint(\"\\n4. TRAINING MULTIPLE MODELS...\")\nprint(\"-\" * 40)\n\n# Define models with class-aware configurations\nmodels = {\n    'Logistic Regression': LogisticRegression(\n        random_state=42, \n        max_iter=1000,\n        class_weight='balanced',\n        C=0.1\n    ),\n    \n    'Random Forest (Balanced)': RandomForestClassifier(\n        n_estimators=200,\n        max_depth=15,\n        min_samples_split=5,\n        min_samples_leaf=2,\n        random_state=42,\n        class_weight='balanced',\n        n_jobs=-1\n    ),\n    \n    'Random Forest (Oversampled)': RandomForestClassifier(\n        n_estimators=200,\n        max_depth=15,\n        min_samples_split=5,\n        min_samples_leaf=2,\n        random_state=42,\n        n_jobs=-1\n    ),\n    \n    'Gradient Boosting': GradientBoostingClassifier(\n        n_estimators=200,\n        learning_rate=0.1,\n        max_depth=6,\n        min_samples_split=5,\n        min_samples_leaf=2,\n        random_state=42\n    )\n}\n\n# Train models and collect results\nmodel_results = {}\ntrained_models = {}\n\n# Create figure for ROC curves\nplt.figure(figsize=(12, 8))\ncolors = ['red', 'blue', 'green', 'orange', 'purple', 'brown']\nroc_data = {}\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"DETAILED MODEL PERFORMANCE RESULTS\")\nprint(\"=\"*80)\n\nfor i, (name, model) in enumerate(models.items()):\n    print(f\"\\nðŸ”¹ TRAINING: {name}\")\n    print(\"-\" * 60)\n    \n    # Use different data based on model type\n    if 'Oversampled' in name:\n        X_train_use, y_train_use = X_train_balanced, y_train_balanced\n    else:\n        X_train_use, y_train_use = X_train_scaled, y_train\n    \n    # Train model\n    model.fit(X_train_use, y_train_use)\n    \n    # Cross-validation on training set\n    cv_scores = cross_val_score(model, X_train_use, y_train_use, \n                               cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n                               scoring='roc_auc', n_jobs=-1)\n    \n    # Predictions on test set\n    y_pred = model.predict(X_test_scaled)\n    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n    \n    # Calculate all metrics\n    accuracy = accuracy_score(y_test, y_pred)\n    precision = precision_score(y_test, y_pred)\n    recall = recall_score(y_test, y_pred)\n    f1 = f1_score(y_test, y_pred)\n    auc_score = roc_auc_score(y_test, y_pred_proba)\n    avg_precision = average_precision_score(y_test, y_pred_proba)\n    \n    # Store results\n    model_results[name] = {\n        'CV_AUC_Mean': cv_scores.mean(),\n        'CV_AUC_Std': cv_scores.std(),\n        'Accuracy': accuracy,\n        'Precision': precision,\n        'Recall': recall,\n        'F1_Score': f1,\n        'AUC': auc_score,\n        'Avg_Precision': avg_precision,\n        'Predictions': y_pred,\n        'Probabilities': y_pred_proba\n    }\n    \n    trained_models[name] = model\n    \n    # Print detailed metrics for this model\n    print(f\"ðŸ“Š PERFORMANCE METRICS:\")\n    print(f\"   âœ… Accuracy:     {accuracy:.4f}\")\n    print(f\"   âœ… Precision:    {precision:.4f}\")\n    print(f\"   âœ… Recall:       {recall:.4f}\")\n    print(f\"   âœ… F1-Score:     {f1:.4f}\")\n    print(f\"   âœ… AUC-ROC:      {auc_score:.4f}\")\n    print(f\"   âœ… CV AUC:       {cv_scores.mean():.4f} (+/- {cv_scores.std()*2:.4f})\")\n    \n    # Calculate and store ROC curve data\n    fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n    roc_data[name] = {'fpr': fpr, 'tpr': tpr, 'auc': auc_score}\n    \n    # Plot ROC curve\n    plt.plot(fpr, tpr, linewidth=2, color=colors[i % len(colors)], \n             label=f'{name} (AUC: {auc_score:.3f})')\n\n# 5. CREATE ENSEMBLE MODEL\nprint(f\"\\nðŸ”¹ TRAINING: Ensemble Model\")\nprint(\"-\" * 60)\n\n# Get predictions from top 3 models\nensemble_probabilities = np.zeros(len(y_test))\ntop_3_models = sorted(model_results.items(), key=lambda x: x[1]['AUC'], reverse=True)[:3]\n\nprint(f\"ðŸ“‹ Top 3 models for ensemble:\")\nfor rank, (name, results) in enumerate(top_3_models, 1):\n    print(f\"   {rank}. {name} (AUC: {results['AUC']:.4f})\")\n\nweights = [0.4, 0.35, 0.25]  # Weighted ensemble\nfor i, (name, results) in enumerate(top_3_models):\n    ensemble_probabilities += weights[i] * results['Probabilities']\n\nensemble_predictions = (ensemble_probabilities > 0.5).astype(int)\n\n# Calculate ensemble metrics\nensemble_accuracy = accuracy_score(y_test, ensemble_predictions)\nensemble_precision = precision_score(y_test, ensemble_predictions)\nensemble_recall = recall_score(y_test, ensemble_predictions)\nensemble_f1 = f1_score(y_test, ensemble_predictions)\nensemble_auc = roc_auc_score(y_test, ensemble_probabilities)\nensemble_avg_precision = average_precision_score(y_test, ensemble_probabilities)\n\nmodel_results['Ensemble (Top 3)'] = {\n    'CV_AUC_Mean': np.mean([r['CV_AUC_Mean'] for _, r in top_3_models]),\n    'CV_AUC_Std': np.mean([r['CV_AUC_Std'] for _, r in top_3_models]),\n    'Accuracy': ensemble_accuracy,\n    'Precision': ensemble_precision,\n    'Recall': ensemble_recall,\n    'F1_Score': ensemble_f1,\n    'AUC': ensemble_auc,\n    'Avg_Precision': ensemble_avg_precision,\n    'Predictions': ensemble_predictions,\n    'Probabilities': ensemble_probabilities\n}\n\nprint(f\"ðŸ“Š ENSEMBLE PERFORMANCE METRICS:\")\nprint(f\"   âœ… Accuracy:     {ensemble_accuracy:.4f}\")\nprint(f\"   âœ… Precision:    {ensemble_precision:.4f}\")\nprint(f\"   âœ… Recall:       {ensemble_recall:.4f}\")\nprint(f\"   âœ… F1-Score:     {ensemble_f1:.4f}\")\nprint(f\"   âœ… AUC-ROC:      {ensemble_auc:.4f}\")\n\n# Add ensemble to ROC plot\nfpr_ensemble, tpr_ensemble, _ = roc_curve(y_test, ensemble_probabilities)\nplt.plot(fpr_ensemble, tpr_ensemble, linewidth=3, color='black', linestyle='--',\n         label=f'Ensemble (AUC: {ensemble_auc:.3f})')\n\n# Finalize ROC plot\nplt.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Random Classifier')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate', fontsize=12)\nplt.ylabel('True Positive Rate', fontsize=12)\nplt.title('ROC Curves - All Models Comparison', fontsize=14, fontweight='bold')\nplt.legend(loc=\"lower right\", fontsize=10)\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n# 6. COMPREHENSIVE MODEL COMPARISON TABLE\nprint(\"\\n\" + \"=\"*80)\nprint(\"COMPREHENSIVE MODEL COMPARISON\")\nprint(\"=\"*80)\n\n# Create detailed comparison DataFrame\ncomparison_df = pd.DataFrame(model_results).T\ncomparison_df = comparison_df[['Accuracy', 'Precision', 'Recall', 'F1_Score', 'AUC', 'CV_AUC_Mean']].round(4)\n\nprint(comparison_df.to_string())\n\n# Find best model for each metric\nprint(f\"\\nðŸ† BEST PERFORMERS BY METRIC:\")\nprint(f\"   ðŸ¥‡ Best Accuracy:  {comparison_df['Accuracy'].idxmax()} ({comparison_df['Accuracy'].max():.4f})\")\nprint(f\"   ðŸ¥‡ Best Precision: {comparison_df['Precision'].idxmax()} ({comparison_df['Precision'].max():.4f})\")\nprint(f\"   ðŸ¥‡ Best Recall:    {comparison_df['Recall'].idxmax()} ({comparison_df['Recall'].max():.4f})\")\nprint(f\"   ðŸ¥‡ Best F1-Score:  {comparison_df['F1_Score'].idxmax()} ({comparison_df['F1_Score'].max():.4f})\")\nprint(f\"   ðŸ¥‡ Best AUC:       {comparison_df['AUC'].idxmax()} ({comparison_df['AUC'].max():.4f})\")\n\n# Overall best model (based on AUC)\nbest_model_name = comparison_df['AUC'].idxmax()\nprint(f\"\\nðŸŽ¯ OVERALL BEST MODEL: {best_model_name}\")\nprint(f\"   ðŸ“ˆ AUC Score: {comparison_df.loc[best_model_name, 'AUC']:.4f}\")\n\n# 7. DETAILED EVALUATION OF BEST MODEL\nprint(f\"\\n\" + \"=\"*80)\nprint(f\"DETAILED ANALYSIS - {best_model_name}\")\nprint(\"=\"*80)\n\nbest_predictions = model_results[best_model_name]['Predictions']\nbest_probabilities = model_results[best_model_name]['Probabilities']\n\n# Classification Report\nprint(\"ðŸ“‹ CLASSIFICATION REPORT:\")\nprint(classification_report(y_test, best_predictions, target_names=['Survived', 'Expired'], digits=4))\n\n# Confusion Matrix\nprint(\"\\nðŸ“Š CONFUSION MATRIX:\")\ncm = confusion_matrix(y_test, best_predictions)\nprint(cm)\n\n# Calculate additional metrics\ntn, fp, fn, tp = cm.ravel()\nspecificity = tn / (tn + fp)\nsensitivity = tp / (tp + fn)\nppv = tp / (tp + fp) if (tp + fp) > 0 else 0\nnpv = tn / (tn + fn) if (tn + fn) > 0 else 0\n\nprint(f\"\\nðŸ“ˆ ADDITIONAL CLINICAL METRICS:\")\nprint(f\"   âœ… Sensitivity (True Positive Rate): {sensitivity:.4f}\")\nprint(f\"   âœ… Specificity (True Negative Rate): {specificity:.4f}\")\nprint(f\"   âœ… Positive Predictive Value (PPV):  {ppv:.4f}\")\nprint(f\"   âœ… Negative Predictive Value (NPV):  {npv:.4f}\")\n\n# 8. SUMMARY TABLE OF ALL METRICS\nprint(f\"\\n\" + \"=\"*80)\nprint(\"FINAL SUMMARY - ALL MODELS WITH ALL METRICS\")\nprint(\"=\"*80)\n\n# Create a more comprehensive summary\nsummary_data = []\nfor model_name, results in model_results.items():\n    summary_data.append({\n        'Model': model_name,\n        'Accuracy': f\"{results['Accuracy']:.4f}\",\n        'Precision': f\"{results['Precision']:.4f}\",\n        'Recall': f\"{results['Recall']:.4f}\",\n        'F1-Score': f\"{results['F1_Score']:.4f}\",\n        'AUC-ROC': f\"{results['AUC']:.4f}\",\n        'CV AUC': f\"{results['CV_AUC_Mean']:.4f}\"\n    })\n\nsummary_df = pd.DataFrame(summary_data)\nprint(summary_df.to_string(index=False))\n\nprint(f\"\\nðŸš€ MODELS READY FOR DEPLOYMENT!\")\nprint(f\"   ðŸ“Š Total models trained: {len(model_results)}\")\nprint(f\"   ðŸŽ¯ Best overall model: {best_model_name}\")\nprint(f\"   ðŸ“ˆ Best AUC achieved: {comparison_df['AUC'].max():.4f}\")\nprint(f\"   ðŸ”„ All metrics calculated and displayed above\")\n\n# Store ROC curve data for potential future use\nprint(f\"\\nðŸ’¾ ROC curve data stored for all models\")\nprint(f\"   ðŸ“ˆ Use 'roc_data' dictionary to access FPR, TPR, and AUC for each model\")","metadata":{"_uuid":"db55c3ed-53c0-45c3-98fe-160a6a589a35","_cell_guid":"784cadec-5481-4964-8b13-6769ea48d073","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-06-03T15:11:03.903349Z","iopub.execute_input":"2025-06-03T15:11:03.903592Z","iopub.status.idle":"2025-06-03T15:14:47.357222Z","shell.execute_reply.started":"2025-06-03T15:11:03.903569Z","shell.execute_reply":"2025-06-03T15:14:47.351573Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Hyperparamter**","metadata":{}},{"cell_type":"code","source":"# Comprehensive Healthcare ML Pipeline - MIMIC-III Dataset Analysis\n# Fixed and Enhanced Version with All Required Metrics and Visualizations\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, learning_curve\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score, \n    confusion_matrix, classification_report, roc_curve, auc, roc_auc_score\n)\n\n# Import all required models\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import (\n    RandomForestClassifier, GradientBoostingClassifier, \n    VotingClassifier, AdaBoostClassifier, BaggingClassifier\n)\nimport xgboost as xgb\nimport lightgbm as lgb\n\n# Suppress warnings for cleaner output\nwarnings.filterwarnings('ignore')\n\nprint(\"=\" * 100)\nprint(\"COMPREHENSIVE HEALTHCARE ML PIPELINE - MIMIC-III DATASET ANALYSIS\")\nprint(\"=\" * 100)\n\n# ============================================================================\n# STEP 1: DATA LOADING AND EXPLORATION\n# ============================================================================\nprint(\"\\nSTEP 1: DATA LOADING AND EXPLORATION\")\nprint(\"-\" * 60)\n\ntry:\n    # Load the dataset - update path as needed\n    data = pd.read_csv('../input/mimic3c/mimic3c.csv')  # Update this path as needed\n    print(f\"âœ“ Dataset loaded successfully!\")\n    print(f\"âœ“ Shape: {data.shape}\")\n    print(f\"âœ“ Columns: {list(data.columns)}\")\n    \n    # Display first few rows\n    print(\"\\nFirst 5 rows preview:\")\n    print(data.head())\n    \n    # Basic statistics\n    print(f\"\\nDataset Info:\")\n    print(f\"- Total samples: {len(data)}\")\n    print(f\"- Total features: {len(data.columns)}\")\n    print(f\"- Missing values: {data.isnull().sum().sum()}\")\n    \n    # Target variable distribution\n    if 'ExpiredHospital' in data.columns:\n        target_dist = data['ExpiredHospital'].value_counts()\n        print(f\"\\nTarget Distribution (ExpiredHospital):\")\n        print(f\"- Survived (0): {target_dist.get(0, 0)} ({target_dist.get(0, 0)/len(data)*100:.1f}%)\")\n        print(f\"- Expired (1): {target_dist.get(1, 0)} ({target_dist.get(1, 0)/len(data)*100:.1f}%)\")\n    \nexcept FileNotFoundError:\n    print(\"âŒ Error: Dataset file not found!\")\n    print(\"Please update the file path in the code to match your data location.\")\n    print(\"Common paths: 'mimic3c.csv', 'data/mimic3c.csv', '../input/mimic3c/mimic3c.csv'\")\n    exit()\n\n# ============================================================================\n# STEP 2: DATA PREPROCESSING\n# ============================================================================\nprint(\"\\n\" + \"=\" * 100)\nprint(\"STEP 2: DATA PREPROCESSING\")\nprint(\"=\" * 100)\n\n# Create a copy for preprocessing\ndf = data.copy()\nprint(f\"Original dataset shape: {df.shape}\")\n\n# 1. HANDLE MISSING VALUES\nprint(\"\\n1. HANDLING MISSING VALUES...\")\nprint(\"-\" * 40)\n\nmissing_before = df.isnull().sum()\nprint(\"Missing values before treatment:\")\nfor col in missing_before[missing_before > 0].index:\n    print(f\"  {col}: {missing_before[col]} ({missing_before[col]/len(df)*100:.1f}%)\")\n\n# Fill missing values based on column type and nature\nif 'marital_status' in df.columns:\n    df['marital_status'].fillna(df['marital_status'].mode()[0] if not df['marital_status'].mode().empty else 'UNKNOWN', inplace=True)\n\nif 'religion' in df.columns:\n    df['religion'].fillna('UNKNOWN', inplace=True)\n\nif 'AdmitDiagnosis' in df.columns:\n    df['AdmitDiagnosis'].fillna(df['AdmitDiagnosis'].mode()[0] if not df['AdmitDiagnosis'].mode().empty else 'UNKNOWN', inplace=True)\n\nprint(f\"âœ“ Missing values after imputation: {df.isnull().sum().sum()}\")\n\n# 2. FEATURE ENGINEERING\nprint(\"\\n2. FEATURE ENGINEERING...\")\nprint(\"-\" * 40)\n\n# Age groups (clinical relevance)\nif 'age' in df.columns:\n    df['age_group'] = pd.cut(df['age'],\n                            bins=[0, 18, 35, 50, 65, 80, 100],\n                            labels=['pediatric', 'young_adult', 'adult', 'middle_aged', 'elderly', 'very_elderly'])\n\n# LOS categories\nif 'LOSdays' in df.columns:\n    df['los_category'] = pd.cut(df['LOSdays'],\n                               bins=[0, 2, 7, 14, 30, 300],\n                               labels=['very_short', 'short', 'medium', 'long', 'very_long'])\n\n# Clinical activity intensity features\nif all(col in df.columns for col in ['NumLabs', 'LOSdays']):\n    df['lab_intensity'] = df['NumLabs'] / (df['LOSdays'] + 1)\nif all(col in df.columns for col in ['NumRx', 'LOSdays']):\n    df['med_intensity'] = df['NumRx'] / (df['LOSdays'] + 1)\nif all(col in df.columns for col in ['NumProcs', 'LOSdays']):\n    df['procedure_intensity'] = df['NumProcs'] / (df['LOSdays'] + 1)\nif all(col in df.columns for col in ['NumChartEvents', 'LOSdays']):\n    df['monitoring_intensity'] = df['NumChartEvents'] / (df['LOSdays'] + 1)\n\n# Total clinical burden score\nburden_cols = ['NumDiagnosis', 'NumProcs', 'NumCallouts']\nif all(col in df.columns for col in burden_cols):\n    df['clinical_burden'] = df[burden_cols].sum(axis=1)\n\n# Interaction density\nif all(col in df.columns for col in ['TotalNumInteract', 'LOSdays']):\n    df['interaction_density'] = df['TotalNumInteract'] / (df['LOSdays'] + 1)\n\n# High-risk indicators\nif 'age' in df.columns:\n    df['high_age'] = (df['age'] >= 70).astype(int)\nif 'LOSdays' in df.columns:\n    df['high_los'] = (df['LOSdays'] >= 14).astype(int)\nif 'NumDiagnosis' in df.columns:\n    df['high_diagnoses'] = (df['NumDiagnosis'] >= df['NumDiagnosis'].quantile(0.75)).astype(int)\nif 'admit_type' in df.columns:\n    df['emergency_admit'] = (df['admit_type'] == 'EMERGENCY').astype(int)\n\n# ICU indicators\nif 'NumChartEvents' in df.columns:\n    df['icu_indicator'] = (df['NumChartEvents'] >= df['NumChartEvents'].quantile(0.90)).astype(int)\n\nnew_features = [col for col in df.columns if col not in data.columns]\nprint(f\"âœ“ New features created: {len(new_features)}\")\nfor feature in new_features[:10]:  # Show first 10\n    print(f\"  - {feature}\")\nif len(new_features) > 10:\n    print(f\"  ... and {len(new_features) - 10} more\")\n\n# 3. ENCODE CATEGORICAL VARIABLES\nprint(\"\\n3. ENCODING CATEGORICAL VARIABLES...\")\nprint(\"-\" * 40)\n\n# Initialize label encoders\nlabel_encoders = {}\n\n# Categorical columns to encode\ncategorical_cols = []\nfor col in ['gender', 'admit_type', 'admit_location', 'insurance', 'religion', \n           'marital_status', 'ethnicity', 'age_group', 'los_category']:\n    if col in df.columns:\n        categorical_cols.append(col)\n\n# Apply label encoding\nfor col in categorical_cols:\n    le = LabelEncoder()\n    df[col + '_encoded'] = le.fit_transform(df[col].astype(str))\n    label_encoders[col] = le\n    print(f\"  âœ“ {col}: {len(le.classes_)} categories -> encoded\")\n\n# 4. HANDLE HIGH CARDINALITY FEATURES\nprint(\"\\n4. HANDLING HIGH CARDINALITY FEATURES...\")\nprint(\"-\" * 40)\n\n# Handle AdmitDiagnosis if present\nif 'AdmitDiagnosis' in df.columns:\n    diagnosis_freq = df['AdmitDiagnosis'].value_counts()\n    top_diagnoses = diagnosis_freq.head(50).index\n    df['AdmitDiagnosis_grouped'] = df['AdmitDiagnosis'].apply(\n        lambda x: x if x in top_diagnoses else 'OTHER'\n    )\n    le_diag = LabelEncoder()\n    df['AdmitDiagnosis_encoded'] = le_diag.fit_transform(df['AdmitDiagnosis_grouped'])\n    print(f\"  âœ“ AdmitDiagnosis: {len(diagnosis_freq)} -> {len(le_diag.classes_)} categories\")\n\n# Handle AdmitProcedure if present\nif 'AdmitProcedure' in df.columns:\n    procedure_freq = df['AdmitProcedure'].value_counts()\n    top_procedures = procedure_freq.head(50).index\n    df['AdmitProcedure_grouped'] = df['AdmitProcedure'].apply(\n        lambda x: x if x in top_procedures else 'OTHER'\n    )\n    le_proc = LabelEncoder()\n    df['AdmitProcedure_encoded'] = le_proc.fit_transform(df['AdmitProcedure_grouped'])\n    print(f\"  âœ“ AdmitProcedure: {len(procedure_freq)} -> {len(le_proc.classes_)} categories\")\n\n# 5. FEATURE SELECTION FOR MODELING\nprint(\"\\n5. PREPARING FEATURE SET...\")\nprint(\"-\" * 40)\n\n# Build feature list dynamically based on available columns\nfeature_columns = []\n\n# Demographics\nfor col in ['age', 'gender_encoded', 'ethnicity_encoded']:\n    if col in df.columns:\n        feature_columns.append(col)\n\n# Admission details\nfor col in ['admit_type_encoded', 'admit_location_encoded', 'insurance_encoded',\n           'religion_encoded', 'marital_status_encoded']:\n    if col in df.columns:\n        feature_columns.append(col)\n\n# Clinical features\nclinical_cols = ['LOSdays', 'NumDiagnosis', 'NumProcs', 'NumCallouts',\n                'NumCPTevents', 'NumInput', 'NumLabs', 'NumMicroLabs',\n                'NumNotes', 'NumOutput', 'NumRx', 'NumProcEvents',\n                'NumTransfers', 'NumChartEvents', 'TotalNumInteract']\nfor col in clinical_cols:\n    if col in df.columns:\n        feature_columns.append(col)\n\n# Encoded diagnosis and procedure\nfor col in ['AdmitDiagnosis_encoded', 'AdmitProcedure_encoded']:\n    if col in df.columns:\n        feature_columns.append(col)\n\n# Engineered features\nengineered_cols = ['age_group_encoded', 'los_category_encoded',\n                  'lab_intensity', 'med_intensity', 'procedure_intensity', 'monitoring_intensity',\n                  'clinical_burden', 'interaction_density',\n                  'high_age', 'high_los', 'high_diagnoses', 'emergency_admit', 'icu_indicator']\nfor col in engineered_cols:\n    if col in df.columns:\n        feature_columns.append(col)\n\n# Create feature matrix\nX = df[feature_columns].copy()\ny = df['ExpiredHospital'].copy()\n\nprint(f\"âœ“ Final feature set: {X.shape[1]} features\")\nprint(f\"âœ“ Target distribution: {y.value_counts().to_dict()}\")\n\n# Store feature names for later use\nfeature_names = X.columns.tolist()\n\n# 6. HANDLE OUTLIERS\nprint(\"\\n6. OUTLIER HANDLING...\")\nprint(\"-\" * 40)\n\ncontinuous_features = []\nfor col in ['LOSdays', 'NumDiagnosis', 'NumInput', 'NumLabs', 'NumChartEvents',\n           'TotalNumInteract', 'lab_intensity', 'med_intensity', 'monitoring_intensity']:\n    if col in X.columns:\n        continuous_features.append(col)\n\noutlier_stats = {}\nfor feature in continuous_features:\n    Q1 = X[feature].quantile(0.25)\n    Q3 = X[feature].quantile(0.75)\n    IQR = Q3 - Q1\n    lower_bound = Q1 - 1.5 * IQR\n    upper_bound = Q3 + 1.5 * IQR\n    \n    outliers_before = ((X[feature] < lower_bound) | (X[feature] > upper_bound)).sum()\n    X[feature] = np.clip(X[feature], lower_bound, upper_bound)\n    outlier_stats[feature] = outliers_before\n\nprint(\"âœ“ Outliers capped per feature:\")\nfor feature, count in outlier_stats.items():\n    if count > 0:\n        print(f\"  {feature}: {count} outliers capped\")\n\n# 7. FINAL DATASET VALIDATION\nprint(f\"\\n7. FINAL PREPROCESSED DATASET:\")\nprint(\"-\" * 40)\nprint(f\"âœ“ Shape: {X.shape}\")\nprint(f\"âœ“ Target distribution:\")\nprint(f\"  - Survived (0): {(y==0).sum()} ({(y==0).sum()/len(y)*100:.1f}%)\")\nprint(f\"  - Expired (1): {(y==1).sum()} ({(y==1).sum()/len(y)*100:.1f}%)\")\nprint(f\"âœ“ Missing values in X: {X.isnull().sum().sum()}\")\nprint(f\"âœ“ Missing values in y: {y.isnull().sum()}\")\nprint(f\"âœ“ Infinite values: {np.isinf(X).sum().sum()}\")\n\n# ============================================================================\n# STEP 3: TRAIN-TEST SPLIT AND SCALING\n# ============================================================================\nprint(\"\\n\" + \"=\" * 100)\nprint(\"STEP 3: TRAIN-TEST SPLIT AND SCALING\")\nprint(\"=\" * 100)\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\nprint(f\"âœ“ Training set: {X_train.shape[0]} samples\")\nprint(f\"âœ“ Test set: {X_test.shape[0]} samples\")\nprint(f\"âœ“ Training class distribution: {y_train.value_counts().to_dict()}\")\nprint(f\"âœ“ Test class distribution: {y_test.value_counts().to_dict()}\")\n\n# Feature scaling\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\nprint(\"âœ“ Features scaled using StandardScaler\")\n\n# Calculate class weights for imbalance handling\nclass_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\nclass_weight_dict = {0: class_weights[0], 1: class_weights[1]}\nscale_pos_weight = class_weights[1] / class_weights[0]\n\nprint(f\"âœ“ Class weights: {class_weight_dict}\")\nprint(f\"âœ“ Scale pos weight: {scale_pos_weight:.3f}\")\n\n# ============================================================================\n# STEP 4: MODEL EVALUATION FUNCTIONS\n# ============================================================================\n\n# Set style for plots\nplt.style.use('default')\nsns.set_palette(\"husl\")\nCLASS_NAMES = ['Survived', 'Expired']\n\ndef evaluate_model_comprehensive(model, X_test, y_test, model_name):\n    \"\"\"Comprehensive model evaluation with all metrics\"\"\"\n    y_pred = model.predict(X_test)\n    \n    # Get prediction probabilities if available\n    if hasattr(model, \"predict_proba\"):\n        y_proba = model.predict_proba(X_test)[:, 1]\n    elif hasattr(model, \"decision_function\"):\n        y_proba = model.decision_function(X_test)\n    else:\n        y_proba = y_pred\n    \n    # Calculate all metrics\n    metrics = {\n        'Model': model_name,\n        'Accuracy': accuracy_score(y_test, y_pred),\n        'Precision': precision_score(y_test, y_pred, average='weighted', zero_division=0),\n        'Recall': recall_score(y_test, y_pred, average='weighted', zero_division=0),\n        'F1-Score': f1_score(y_test, y_pred, average='weighted', zero_division=0),\n        'ROC-AUC': roc_auc_score(y_test, y_proba) if len(np.unique(y_test)) > 1 else 0.0\n    }\n    \n    return metrics, y_pred, y_proba\n\ndef plot_comprehensive_results(results_df):\n    \"\"\"Create comprehensive visualization of all results\"\"\"\n    fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n    \n    # 1. Accuracy comparison\n    results_df_sorted = results_df.sort_values('Accuracy', ascending=True)\n    axes[0, 0].barh(results_df_sorted['Model'], results_df_sorted['Accuracy'], color='skyblue')\n    axes[0, 0].set_xlabel('Accuracy')\n    axes[0, 0].set_title('Model Accuracy Comparison')\n    axes[0, 0].set_xlim(0, 1)\n    for i, v in enumerate(results_df_sorted['Accuracy']):\n        axes[0, 0].text(v + 0.01, i, f'{v:.3f}', va='center', fontsize=8)\n    \n    # 2. F1-Score comparison\n    results_df_sorted = results_df.sort_values('F1-Score', ascending=True)\n    axes[0, 1].barh(results_df_sorted['Model'], results_df_sorted['F1-Score'], color='lightcoral')\n    axes[0, 1].set_xlabel('F1-Score')\n    axes[0, 1].set_title('Model F1-Score Comparison')\n    axes[0, 1].set_xlim(0, 1)\n    for i, v in enumerate(results_df_sorted['F1-Score']):\n        axes[0, 1].text(v + 0.01, i, f'{v:.3f}', va='center', fontsize=8)\n    \n    # 3. ROC-AUC comparison\n    results_df_sorted = results_df.sort_values('ROC-AUC', ascending=True)\n    axes[0, 2].barh(results_df_sorted['Model'], results_df_sorted['ROC-AUC'], color='lightgreen')\n    axes[0, 2].set_xlabel('ROC-AUC')\n    axes[0, 2].set_title('Model ROC-AUC Comparison')\n    axes[0, 2].set_xlim(0, 1)\n    for i, v in enumerate(results_df_sorted['ROC-AUC']):\n        axes[0, 2].text(v + 0.01, i, f'{v:.3f}', va='center', fontsize=8)\n    \n    # 4. Precision vs Recall scatter plot\n    axes[1, 0].scatter(results_df['Recall'], results_df['Precision'], \n                      s=100, alpha=0.7, c=range(len(results_df)), cmap='viridis')\n    axes[1, 0].set_xlabel('Recall')\n    axes[1, 0].set_ylabel('Precision')\n    axes[1, 0].set_title('Precision vs Recall')\n    axes[1, 0].grid(True, alpha=0.3)\n    for i, model in enumerate(results_df['Model']):\n        axes[1, 0].annotate(model[:15], (results_df['Recall'].iloc[i], results_df['Precision'].iloc[i]),\n                           xytext=(5, 5), textcoords='offset points', fontsize=7, rotation=15)\n    \n    # 5. All metrics comparison for top 5 models\n    top_5_models = results_df.nlargest(5, 'Accuracy')\n    metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']\n    \n    x = np.arange(len(metrics))\n    width = 0.15\n    \n    for i, (idx, row) in enumerate(top_5_models.iterrows()):\n        values = [row[metric] for metric in metrics]\n        axes[1, 1].bar(x + i * width, values, width, label=row['Model'][:12], alpha=0.8)\n    \n    axes[1, 1].set_xlabel('Metrics')\n    axes[1, 1].set_ylabel('Score')\n    axes[1, 1].set_title('Top 5 Models - All Metrics Comparison')\n    axes[1, 1].set_xticks(x + width * 2)\n    axes[1, 1].set_xticklabels(metrics, rotation=45)\n    axes[1, 1].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n    axes[1, 1].grid(True, alpha=0.3)\n    axes[1, 1].set_ylim(0, 1.1)\n    \n    # 6. Model ranking heatmap\n    ranking_data = results_df[['Model', 'Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']].set_index('Model')\n    im = axes[1, 2].imshow(ranking_data.values, cmap='RdYlGn', aspect='auto', vmin=0, vmax=1)\n    axes[1, 2].set_xticks(range(len(ranking_data.columns)))\n    axes[1, 2].set_xticklabels(ranking_data.columns, rotation=45)\n    axes[1, 2].set_yticks(range(len(ranking_data.index)))\n    axes[1, 2].set_yticklabels([name[:15] for name in ranking_data.index])\n    axes[1, 2].set_title('Model Performance Heatmap')\n    \n    # Add colorbar\n    cbar = plt.colorbar(im, ax=axes[1, 2])\n    cbar.set_label('Score')\n    \n    # Add text annotations\n    for i in range(len(ranking_data.index)):\n        for j in range(len(ranking_data.columns)):\n            text = axes[1, 2].text(j, i, f'{ranking_data.iloc[i, j]:.2f}',\n                                 ha=\"center\", va=\"center\", color=\"black\", fontsize=7)\n    \n    plt.tight_layout()\n    plt.show()\n\ndef plot_roc_curves_comprehensive(models_results, X_test, y_test):\n    \"\"\"Plot ROC curves for all models\"\"\"\n    plt.figure(figsize=(12, 8))\n    \n    for model_name, (model, _, y_proba) in models_results.items():\n        if len(np.unique(y_test)) > 1:\n            fpr, tpr, _ = roc_curve(y_test, y_proba)\n            roc_auc = auc(fpr, tpr)\n            plt.plot(fpr, tpr, label=f'{model_name} (AUC = {roc_auc:.3f})', linewidth=2)\n    \n    plt.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random Classifier')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate', fontsize=12)\n    plt.ylabel('True Positive Rate', fontsize=12)\n    plt.title('ROC Curves Comparison - All Models', fontsize=14)\n    plt.legend(loc=\"lower right\")\n    plt.grid(True, alpha=0.3)\n    plt.show()\n\ndef plot_confusion_matrices_comprehensive(models_results, y_test):\n    \"\"\"Plot confusion matrices for top 6 models\"\"\"\n    models_list = list(models_results.items())[:6]\n    n_models = len(models_list)\n    \n    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n    axes = axes.ravel()\n    \n    for i, (model_name, (model, y_pred, _)) in enumerate(models_list):\n        cm = confusion_matrix(y_test, y_pred)\n        \n        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[i],\n                   xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES)\n        axes[i].set_title(f'Confusion Matrix - {model_name}', fontsize=12)\n        axes[i].set_xlabel('Predicted')\n        axes[i].set_ylabel('Actual')\n    \n    # Hide unused subplots\n    for i in range(n_models, len(axes)):\n        axes[i].set_visible(False)\n    \n    plt.tight_layout()\n    plt.show()\n\n# ============================================================================\n# STEP 5: MODEL TRAINING AND HYPERPARAMETER TUNING\n# ============================================================================\nprint(\"\\n\" + \"=\" * 100)\nprint(\"STEP 5: MODEL TRAINING AND HYPERPARAMETER TUNING\")\nprint(\"=\" * 100)\n\n# Define models with parameter grids\nmodels_params = {\n    'Random Forest': {\n        'model': RandomForestClassifier(random_state=42, class_weight='balanced'),\n        'params': {\n            'n_estimators': [100, 200],\n            'max_depth': [5, 10, None],\n            'min_samples_split': [2, 5]\n        }\n    },\n    'XGBoost': {\n        'model': xgb.XGBClassifier(random_state=42, eval_metric='logloss'),\n        'params': {\n            'n_estimators': [100, 200],\n            'max_depth': [3, 4, 5],\n            'learning_rate': [0.1, 0.2],\n            'scale_pos_weight': [scale_pos_weight]\n        }\n    },\n    'LightGBM': {\n        'model': lgb.LGBMClassifier(random_state=42, verbose=-1, class_weight='balanced'),\n        'params': {\n            'n_estimators': [100, 200],\n            'max_depth': [3, 4, 5],\n            'learning_rate': [0.1, 0.2]\n        }\n    },\n    'Gradient Boosting': {\n        'model': GradientBoostingClassifier(random_state=42),\n        'params': {\n            'n_estimators': [100, 200],\n            'max_depth': [3, 4],\n            'learning_rate': [0.1, 0.2]\n        }\n    },\n    'Support Vector Machine': {\n        'model': SVC(random_state=42, probability=True, class_weight='balanced'),\n        'params': {\n            'C': [0.1, 1, 10],\n            'kernel': ['linear', 'rbf']\n        }\n    },\n    'Neural Network': {\n        'model': MLPClassifier(random_state=42, max_iter=1000),\n        'params': {\n            'hidden_layer_sizes': [(50,), (100,), (50, 50)],\n            'activation': ['relu', 'tanh'],\n            'alpha': [0.0001, 0.001]\n        }\n    }\n}\n\n# Store results\ntuned_models = {}\nall_results = []\nmodel_predictions = {}\n\nprint(\"Starting hyperparameter tuning...\")\n\nfor model_name, model_info in models_params.items():\n    print(f\"\\nðŸ”§ Tuning {model_name}...\")\n    \n    try:\n        # Grid search with cross-validation\n        grid_search = GridSearchCV(\n            model_info['model'], \n            model_info['params'], \n            cv=3,  # Reduced for faster execution\n            scoring='roc_auc',\n            n_jobs=-1,\n            verbose=0\n        )\n        \n        grid_search.fit(X_train_scaled, y_train)\n        best_model = grid_search.best_estimator_\n        tuned_models[model_name] = best_model\n        \n        # Comprehensive evaluation\n        metrics, y_pred, y_proba = evaluate_model_comprehensive(best_model, X_test_scaled, y_test, model_name)\n        all_results.append(metrics)\n        model_predictions[model_name] = (best_model, y_pred, y_proba)\n        \n        print(f\"  âœ“ Best params: {grid_search.best_params_}\")\n        print(f\"  âœ“ Best CV score: {grid_search.best_score_:.4f}\")\n        print(f\"  âœ“ Test ROC-AUC: {metrics['ROC-AUC']:.4f}\")\n        \n    except Exception as e:\n        print(f\"  âŒ Error training {model_name}: {str(e)}\")\n        continue\n\n# ============================================================================\n# STEP 6: ENSEMBLE METHODS (CONTINUED)\n# ============================================================================\n\n# Create ensemble models\nensemble_models = {}\n\ntry:\n    # 1. Voting Classifier (Soft)\n    print(\"ðŸ”§ Training Voting Classifier...\")\n    voting_estimators = [(name.replace(' ', '_').lower(), model) for name, model in tuned_models.items()]\n    \n    voting_soft = VotingClassifier(\n        estimators=voting_estimators,\n        voting='soft'\n    )\n    voting_soft.fit(X_train_scaled, y_train)\n    ensemble_models['Voting Classifier (Soft)'] = voting_soft\n    \n    # Evaluate\n    metrics, y_pred, y_proba = evaluate_model_comprehensive(voting_soft, X_test_scaled, y_test, 'Voting Classifier (Soft)')\n    all_results.append(metrics)\n    model_predictions['Voting Classifier (Soft)'] = (voting_soft, y_pred, y_proba)\n    print(f\"  âœ“ Voting Classifier ROC-AUC: {metrics['ROC-AUC']:.4f}\")\n    \nexcept Exception as e:\n    print(f\"  âŒ Error training Voting Classifier: {str(e)}\")\n\ntry:\n    # 2. AdaBoost\n    print(\"ðŸ”§ Training AdaBoost...\")\n    ada_boost = AdaBoostClassifier(\n        n_estimators=100,\n        learning_rate=1.0,\n        random_state=42\n    )\n    ada_boost.fit(X_train_scaled, y_train)\n    ensemble_models['AdaBoost'] = ada_boost\n    \n    # Evaluate\n    metrics, y_pred, y_proba = evaluate_model_comprehensive(ada_boost, X_test_scaled, y_test, 'AdaBoost')\n    all_results.append(metrics)\n    model_predictions['AdaBoost'] = (ada_boost, y_pred, y_proba)\n    print(f\"  âœ“ AdaBoost ROC-AUC: {metrics['ROC-AUC']:.4f}\")\n    \nexcept Exception as e:\n    print(f\"  âŒ Error training AdaBoost: {str(e)}\")\n\ntry:\n    # 3. Bagging Classifier\n    print(\"ðŸ”§ Training Bagging Classifier...\")\n    bagging = BaggingClassifier(\n        n_estimators=100,\n        random_state=42,\n        n_jobs=-1\n    )\n    bagging.fit(X_train_scaled, y_train)\n    ensemble_models['Bagging Classifier'] = bagging\n    \n    # Evaluate\n    metrics, y_pred, y_proba = evaluate_model_comprehensive(bagging, X_test_scaled, y_test, 'Bagging Classifier')\n    all_results.append(metrics)\n    model_predictions['Bagging Classifier'] = (bagging, y_pred, y_proba)\n    print(f\"  âœ“ Bagging Classifier ROC-AUC: {metrics['ROC-AUC']:.4f}\")\n    \nexcept Exception as e:\n    print(f\"  âŒ Error training Bagging Classifier: {str(e)}\")\n\n\n\n# ============================================================================\n# STEP 8: RESULTS COMPILATION AND ANALYSIS\n# ============================================================================\nprint(\"\\n\" + \"=\" * 100)\nprint(\"STEP 8: COMPREHENSIVE RESULTS ANALYSIS\")\nprint(\"=\" * 100)\n\n# Create results DataFrame\nresults_df = pd.DataFrame(all_results)\nresults_df = results_df.sort_values('ROC-AUC', ascending=False).reset_index(drop=True)\n\nprint(\"ðŸ“Š FINAL MODEL PERFORMANCE SUMMARY:\")\nprint(\"=\" * 80)\nprint(results_df.to_string(index=False, float_format='%.4f'))\n\n# Top 3 models\nprint(f\"\\nðŸ† TOP 3 MODELS:\")\nprint(\"-\" * 40)\nfor i, (idx, row) in enumerate(results_df.head(3).iterrows(), 1):\n    print(f\"{i}. {row['Model']}\")\n    print(f\"   ROC-AUC: {row['ROC-AUC']:.4f} | Accuracy: {row['Accuracy']:.4f} | F1-Score: {row['F1-Score']:.4f}\")\n\n# ============================================================================\n# STEP 9: DETAILED ANALYSIS OF BEST MODEL\n# ============================================================================\nprint(\"\\n\" + \"=\" * 100)\nprint(\"STEP 9: DETAILED ANALYSIS OF BEST MODEL\")\nprint(\"=\" * 100)\n\nbest_model_name = results_df.iloc[0]['Model']\nbest_model = model_predictions[best_model_name][0]\nbest_y_pred = model_predictions[best_model_name][1]\n\nprint(f\"ðŸŽ¯ BEST MODEL: {best_model_name}\")\nprint(f\"ðŸ” DETAILED PERFORMANCE ANALYSIS:\")\nprint(\"-\" * 60)\n\n# Classification report\nprint(\"Classification Report:\")\nprint(classification_report(y_test, best_y_pred, target_names=CLASS_NAMES))\n\n# Confusion matrix details\ncm = confusion_matrix(y_test, best_y_pred)\ntn, fp, fn, tp = cm.ravel()\n\nprint(f\"\\nConfusion Matrix Breakdown:\")\nprint(f\"True Negatives (Correctly predicted survivors): {tn}\")\nprint(f\"False Positives (Incorrectly predicted deaths): {fp}\")\nprint(f\"False Negatives (Missed deaths): {fn}\")\nprint(f\"True Positives (Correctly predicted deaths): {tp}\")\n\n# Additional metrics\nspecificity = tn / (tn + fp) if (tn + fp) > 0 else 0\nsensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\nprecision_class_1 = tp / (tp + fp) if (tp + fp) > 0 else 0\n\nprint(f\"\\nAdditional Clinical Metrics:\")\nprint(f\"Sensitivity (Recall for deaths): {sensitivity:.4f}\")\nprint(f\"Specificity (Recall for survivors): {specificity:.4f}\")\nprint(f\"Precision for death prediction: {precision_class_1:.4f}\")\n\n# ============================================================================\n# STEP 10: FEATURE IMPORTANCE ANALYSIS\n# ============================================================================\nprint(\"\\n\" + \"=\" * 100)\nprint(\"STEP 10: FEATURE IMPORTANCE ANALYSIS\")\nprint(\"=\" * 100)\n\n# Feature importance for tree-based models\nif hasattr(best_model, 'feature_importances_'):\n    print(f\"ðŸŒŸ FEATURE IMPORTANCE ANALYSIS ({best_model_name}):\")\n    print(\"-\" * 60)\n    \n    # Get feature importances\n    feature_importance = pd.DataFrame({\n        'feature': feature_names,\n        'importance': best_model.feature_importances_\n    }).sort_values('importance', ascending=False)\n    \n    print(\"Top 15 Most Important Features:\")\n    for i, (idx, row) in enumerate(feature_importance.head(15).iterrows(), 1):\n        print(f\"{i:2d}. {row['feature']:<25} {row['importance']:.4f}\")\n    \n    # Plot feature importance\n    plt.figure(figsize=(12, 8))\n    top_features = feature_importance.head(20)\n    plt.barh(range(len(top_features)), top_features['importance'])\n    plt.yticks(range(len(top_features)), top_features['feature'])\n    plt.xlabel('Feature Importance')\n    plt.title(f'Top 20 Feature Importances - {best_model_name}')\n    plt.gca().invert_yaxis()\n    plt.tight_layout()\n    plt.show()\n\nelif hasattr(best_model, 'coef_'):\n    print(f\"ðŸŒŸ FEATURE COEFFICIENTS ANALYSIS ({best_model_name}):\")\n    print(\"-\" * 60)\n    \n    # Get feature coefficients\n    if len(best_model.coef_.shape) > 1:\n        coefficients = best_model.coef_[0]\n    else:\n        coefficients = best_model.coef_\n    \n    feature_coeff = pd.DataFrame({\n        'feature': feature_names,\n        'coefficient': coefficients,\n        'abs_coefficient': np.abs(coefficients)\n    }).sort_values('abs_coefficient', ascending=False)\n    \n    print(\"Top 15 Most Important Features (by absolute coefficient):\")\n    for i, (idx, row) in enumerate(feature_coeff.head(15).iterrows(), 1):\n        print(f\"{i:2d}. {row['feature']:<25} {row['coefficient']:8.4f}\")\n\n# ============================================================================\n# STEP 11: COMPREHENSIVE VISUALIZATIONS\n# ============================================================================\nprint(\"\\n\" + \"=\" * 100)\nprint(\"STEP 11: COMPREHENSIVE VISUALIZATIONS\")\nprint(\"=\" * 100)\n\n# 1. Comprehensive results visualization\nprint(\"ðŸ“ˆ Generating comprehensive performance comparison...\")\nplot_comprehensive_results(results_df)\n\n# 2. ROC curves for all models\nprint(\"ðŸ“ˆ Generating ROC curves comparison...\")\nplot_roc_curves_comprehensive(model_predictions, X_test_scaled, y_test)\n\n# 3. Confusion matrices for top models\nprint(\"ðŸ“ˆ Generating confusion matrices...\")\nplot_confusion_matrices_comprehensive(model_predictions, y_test)\n\n# 4. Learning curves for best model\nprint(\"ðŸ“ˆ Generating learning curve for best model...\")\nif best_model_name in tuned_models or best_model_name in ensemble_models:\n    train_sizes, train_scores, val_scores = learning_curve(\n        best_model, X_train_scaled, y_train, cv=5, n_jobs=-1,\n        train_sizes=np.linspace(0.1, 1.0, 10), scoring='roc_auc'\n    )\n    \n    train_mean = np.mean(train_scores, axis=1)\n    train_std = np.std(train_scores, axis=1)\n    val_mean = np.mean(val_scores, axis=1)\n    val_std = np.std(val_scores, axis=1)\n    \n    plt.figure(figsize=(10, 6))\n    plt.plot(train_sizes, train_mean, 'o-', label='Training Score', color='blue')\n    plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.2, color='blue')\n    plt.plot(train_sizes, val_mean, 'o-', label='Validation Score', color='red')\n    plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.2, color='red')\n    plt.xlabel('Training Set Size')\n    plt.ylabel('ROC-AUC Score')\n    plt.title(f'Learning Curve - {best_model_name}')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.show()\n\n# ============================================================================\n# STEP 12: CROSS-VALIDATION ANALYSIS\n# ============================================================================\nprint(\"\\n\" + \"=\" * 100)\nprint(\"STEP 12: CROSS-VALIDATION ANALYSIS\")\nprint(\"=\" * 100)\n\nprint(\"ðŸ”„ Performing comprehensive cross-validation analysis...\")\n\n# Cross-validation for top 5 models\ncv_results = {}\nfor model_name in results_df.head(5)['Model']:\n    if model_name in model_predictions:\n        model = model_predictions[model_name][0]\n        try:\n            cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='roc_auc', n_jobs=-1)\n            cv_results[model_name] = {\n                'mean': cv_scores.mean(),\n                'std': cv_scores.std(),\n                'scores': cv_scores\n            }\n            print(f\"{model_name:<25} CV ROC-AUC: {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}\")\n        except Exception as e:\n            print(f\"âŒ CV Error for {model_name}: {str(e)}\")\n\n# Visualize CV results\nif cv_results:\n    plt.figure(figsize=(12, 6))\n    models_cv = list(cv_results.keys())\n    cv_means = [cv_results[model]['mean'] for model in models_cv]\n    cv_stds = [cv_results[model]['std'] for model in models_cv]\n    \n    plt.bar(models_cv, cv_means, yerr=cv_stds, capsize=5, alpha=0.7, color='lightblue')\n    plt.xlabel('Models')\n    plt.ylabel('Cross-Validation ROC-AUC Score')\n    plt.title('Cross-Validation Performance Comparison (Top 5 Models)')\n    plt.xticks(rotation=45, ha='right')\n    plt.grid(True, alpha=0.3)\n    \n    # Add value labels on bars\n    for i, (mean, std) in enumerate(zip(cv_means, cv_stds)):\n        plt.text(i, mean + std + 0.01, f'{mean:.3f}', ha='center', va='bottom')\n    \n    plt.tight_layout()\n    plt.show()\n\n# ============================================================================\n# STEP 13: FINAL SUMMARY AND RECOMMENDATIONS\n# ============================================================================\nprint(\"\\n\" + \"=\" * 100)\nprint(\"FINAL SUMMARY AND CLINICAL RECOMMENDATIONS\")\nprint(\"=\" * 100)\n\nprint(\"ðŸŽ¯ EXECUTIVE SUMMARY:\")\nprint(\"-\" * 60)\nprint(f\"âœ“ Total models evaluated: {len(all_results)}\")\nprint(f\"âœ“ Best performing model: {best_model_name}\")\nprint(f\"âœ“ Best ROC-AUC score: {results_df.iloc[0]['ROC-AUC']:.4f}\")\nprint(f\"âœ“ Best accuracy: {results_df.iloc[0]['Accuracy']:.4f}\")\nprint(f\"âœ“ Dataset size: {len(df)} patients\")\nprint(f\"âœ“ Features used: {len(feature_names)}\")\n\nprint(f\"\\nðŸ¥ CLINICAL IMPLICATIONS:\")\nprint(\"-\" * 60)\nprint(f\"â€¢ The model can predict hospital mortality with {results_df.iloc[0]['Accuracy']*100:.1f}% accuracy\")\nprint(f\"â€¢ Sensitivity (death detection): {sensitivity*100:.1f}%\")\nprint(f\"â€¢ Specificity (survivor detection): {specificity*100:.1f}%\")\nprint(f\"â€¢ This tool can assist clinicians in identifying high-risk patients\")\n\n\n\nprint(f\"\\nâœ… ANALYSIS COMPLETE!\")\nprint(\"=\" * 100)\n\n# Save results to file (optional)\ntry:\n    results_df.to_csv('healthcare_ml_results.csv', index=False)\n    print(\"ðŸ“ Results saved to 'healthcare_ml_results.csv'\")\nexcept:\n    print(\"ðŸ“ Could not save results to file\")\n\nprint(\"\\nðŸŽ‰ Healthcare ML Pipeline Analysis Successfully Completed!\")\nprint(\"=\" * 100)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}